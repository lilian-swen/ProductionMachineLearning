**Lilian Sun | Software Engineer** 

*Specializing in Machine Learning, NLP, MLOps, Data Engineering, Agile, DevOps, Microservices*

In this article, I've shared some work experiences related to traditional production machine learning system infrastructure and architecture design. Most of the machine learning projects I participated in focused on traditional machine learning (statistical machine learning) and were situated in B2B fields. My coworkers and I regarded these projects as common enterprise-level web applications.



# Who Conducts ML Infrastructure & Architecture Design 

I worked in a small multifunctional R&D team, as one of the machine learning developers. We handle the entire ML architecture design, technology selection and cost analysis based on data scale, system scale, upstream system architecture, in-house available IT hardware & software resources, annual IT department cost budget and the expertise of our professionals.

In the first two companies where I worked, the system architecture design was typically a collaborative effort between senior developers and the DevOps team. Occasionally, the CTO would become involved to offer insights and suggestions, depending on the scale and complexity of the production machine learning system.

**Ideally, who engage in ML architecture design:**

- **Machine Learning Engineers:** They play a central role in designing and implementing the ML architecture, and therefore possess deep understanding of its technical aspects. They are responsible for assessing the technical feasibility of scaling the architecture and estimating the associated computational resources required.
- **Data Scientists:** They contribute by analyzing the data volume and complexity, which directly impacts the storage and processing needs. They can also provide insights into potential challenges and bottlenecks that might arise during scaling.
- **MLOps Engineers:** They bring expertise in deploying and managing ML systems at scale. They can assess the operational costs associated with different infrastructure options and suggest efficient deployment strategies.
- **Architects and System Administrators:** They provide guidance on the available infrastructure resources, potential limitations, and integration with existing systems. Their insights are crucial for ensuring the scalability of the ML architecture within the broader IT landscape.
- **Project Managers and Business Stakeholders:** They contribute by defining the project budget, performance expectations, and desired scalability goals. Their input helps prioritize features and functionalities based on cost-effectiveness and business value.

In small multifunctional R&D teams I worked in before, the machine learning engineer takes on multiple roles, including Machine Learning Engineer (MLE), Data Scientist, and MLOps Engineer, to assume the primary responsibilities for ML architecture design.



## When to Conduct ML Architecture Design?

Ideally, machine learning architecture design and cost analysis should be performed **throughout the entire machine learning project lifecycle**. However, in the traditional machine learning projects I have worked on, the prototype of the system architecture has typically been designed prior to the commencement of development, often after the demo implementation. Changes to the system architecture are infrequent during the middle of the project unless a performance bottleneck is encountered during the initial phases of development and testing.

**1. Early Project Planning:**

- **Defining the problem and goals:** This initial stage involves understanding the business problem that the ML system aims to solve and outlining the desired outcomes. This information forms the foundation for designing an architecture that aligns with these goals.
- **Data exploration and analysis:** Analyzing the available data helps determine its characteristics, volume, and complexity. This information is crucial for choosing appropriate algorithms and designing data pipelines that can handle the data efficiently.

**2. Design and Development Phase:**

- **Model selection and algorithm choice:** Based on the problem definition and data analysis, suitable machine learning algorithms and models are chosen. The architecture needs to accommodate the chosen algorithms and their specific requirements.
- **Designing the data pipeline:** This involves defining the steps for data ingestion, cleaning, transformation, and feature engineering. The architecture needs to incorporate components for handling these data processing tasks.
- **Training and evaluation infrastructure:** This stage involves setting up the infrastructure for training and evaluating the ML model. The architecture needs to specify the hardware, software, and tools required for this process.

**3. Pre-Deployment and Deployment:**

- **Scalability and cost analysis:** Before deploying the model to production, the architecture is evaluated for its ability to scale and its associated costs. This analysis helps optimize resource allocation and ensure efficient operation under real-world conditions.
- **Deployment planning and orchestration:** The architecture needs to define how the model will be deployed into production, including considerations for serving infrastructure, integration with existing systems, and monitoring pipelines.

**4. Ongoing Maintenance and Monitoring:**

- **Performance monitoring and optimization:** The deployed ML system needs to be continuously monitored for performance, ensuring it meets the desired metrics. The architecture should facilitate easy integration of monitoring tools and enable performance optimization as needed.

- **Model retraining and updates:** As new data becomes available or the business requirements evolve, the model may need to be retrained. The architecture should be flexible enough to accommodate these updates efficiently.

  

# How to Perform Cost Analysis for Production ML Systems

In the machine learning projects I worked on, we firstly estimate the user scale, system scale, data scale based on upstream system architecture, the number of valid users (for B2B projects, this can be calculated from the organization's HR system), the business requirement documents (BRD) and different meetings with stakeholders.

1. **User Scale**: This refers to the number of users who will interact with the system or benefit from its outputs. Understanding the user scale helps in designing user interfaces, managing system load, and ensuring scalability.
2. **System Scale**: System scale encompasses the infrastructure and computational resources required to support the machine learning system. This includes factors such as the number of servers, processing power, memory requirements, and networking capabilities.
3. **Data Scale**: Data scale pertains to the volume, variety, velocity, and veracity of data that the system will process. This involves estimating the size of the datasets, frequency of updates, data quality, and any preprocessing or cleaning steps needed.

Then, we ran the machine learning models on different hardware configurations and analyzing resource utilization to gain valuable insights into how the machine learning model scales under varying workloads. Below is the basic approaches we performed cost analysis for production ML systems:

**1. Benchmarking and Profiling:** Running the model on different hardware configurations and analyzing resource utilization helps estimate the scaling requirements for increased workloads.

**2. Cloud Cost Estimation Tools:** Cloud platforms like AWS, Azure, and GCP offer tools to estimate the cost of running ML workloads based on resource usage patterns. 

For example, we are training an image classification model on a large dataset using a p3.2xlarge instance type. The training is expected to run for 4 hours. We plan to deploy the model for real-time inference using an ml.m5.large instance type. We anticipate 10,000 inference requests per hour. 

Cost Estimation of the image classification model example is shown as below:

**(1) Training Cost:**

- Use the **AWS Pricing Calculator** (https://aws.amazon.com/sagemaker/pricing/) and select "Amazon SageMaker" under "Service."
- Choose "p3.2xlarge" for instance type and enter the estimated training duration (4 hours).
- The calculator will show the estimated on-demand cost for training, which could be around $16 per hour.
- Multiply the hourly cost by the training duration to get the total training cost: $16/hour * 4 hours = $64.

**(2) Inference Cost:**

- Select "ml.m5.large" instance type in the pricing calculator and enter the estimated inference requests per hour (10,000).
- The calculator will show the estimated cost per inference, which could be around $0.00007 per request.
- Calculate the total inference cost for one hour: $0.00007/request * 10,000 requests/hour = $0.70 per hour.

**Total Cost:**

- Combine the training and inference costs: $64 (training) + $0.70/hour (inference) * 24 hours (assuming inference runs for 24 hours) = $70.40

**3. MLOps Platforms:** Some MLOps platforms provide built-in features for cost monitoring and optimization, offering insights into resource consumption and suggesting cost-saving strategies. 

For example, Amazon SageMaker provides built-in features for cost monitoring and optimization within its MLOps capabilities. 

**(1). SageMaker Cost Explorer:**

- This integrated tool within SageMaker provides detailed insights into resource utilization and associated costs for the training jobs, models, and endpoints.
- It enables us to:
  - Visualize costs by different dimensions like instance type, training duration, or endpoint usage.
  - Identify cost outliers and investigate potential cost drivers.
  - Compare costs across different training runs or deployments.
  - Track cost trends over time and monitor budget adherence.

**(2). Cost-Saving Recommendations:**

- SageMaker analyzes the usage patterns and suggests cost-saving strategies, including:
  - **Rightsizing Resources:** Recommends switching to more cost-effective instance types based on the workload requirements.
  - **Spot Instances:** Suggests utilizing cheaper Spot Instances for training jobs that can tolerate interruptions.
  - **Reserved Instances and Savings Plans:** Recommends purchasing reserved instances or savings plans for predictable workloads to get significant discounts.
  - **Stopping Idle Endpoints:** Identifies unused endpoints and recommends stopping them to avoid unnecessary costs.

**(3). Cost-Aware Training:**

- SageMaker allows us to set cost constraints for the training jobs.
- We can specify a maximum budget or cost per hour, and SageMaker will automatically stop the job if it exceeds the limit.
- This helps prevent runaway costs and ensures the training stays within budget.

**(4). Integration with AWS Cost Management:**

- SageMaker Cost Explorer integrates seamlessly with broader AWS Cost Management services.
- This allows us to:
  - Set up budgets and alerts for the overall AWS spending, including SageMaker costs.
  - Leverage cost allocation tags to categorize costs across different projects or teams.
  - Utilize cost forecasting tools to predict future spending trends and make informed resource allocation decisions.

The examples above I mentioned are simplified. The actual cost varies depending on:

- Specific instance types chosen.
- Spot Instances vs. On-Demand Instances pricing.
- Use of Reserved Instances or Savings Plans for discounts.
- Storage costs for data and models.
- Network traffic costs.

**Note:**

- **Amazon SageMaker Cost Explorer:** Provides detailed insights into your SageMaker resource usage and costs.
- **AWS Cost Management:** Offers tools for budgeting, forecasting, and optimizing your overall AWS spending.



## Typical Timeframe for Completing the ML Architecture Analysis

The time required for ML architecture analysis varies significantly depending on data scale, system scale, upstream system architecture, in-house available IT hardware & software resources, annual IT department cost budget and the expertise of our professionals. It can range from **a few days for simpler projects to several weeks or even months for complex systems with intricate requirements**. 

For example:

- **Project complexity:** Larger and more complex projects naturally require more in-depth analysis, leading to a longer timeframe. Some projects took my team 3 months to complete infrastructure and architecture analysis due to the complexity of business requirements and the need for clarity regarding data and system scale. For example, the business team spent 2 months working on gathering business document requirements, contracts, and participating in various meetings to provide the R&D team with essential information for infrastructure and architecture analysis.
- **Data and model size:** Analyzing large datasets and complex models often involves more computational resources and time.
- **Available resources:** The time dedicated to the analysis depends on the available resources within the team.
- **Desired level of detail:** A high-level analysis can be completed quickly, while a more comprehensive and nuanced analysis will take longer.

